{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prgs_bar(dt0, idx, runs, scale=20):\n",
    "    import sys\n",
    "\n",
    "    sys.stdout.write('\\r'+str(idx+1) + '/'+str(runs) + ' ' +\n",
    "                     (' ' if idx+1<runs else '') +\n",
    "                     \" Progress {:2.2%}\".format((idx+1) / runs) + ' |' +\n",
    "                     ''.join(['=']*int(((idx+1) / runs)*scale)) + \n",
    "                     ('>' if idx+1<runs else '') +\n",
    "                     ''.join(['-']*int(scale-int(((idx+1) / runs)*scale)-1)) +\n",
    "                     '|ETA: ' + str((runs-idx)*(datetime.datetime.now()-dt0)/(idx+1))[:4] + '/' +\n",
    "                     str((runs)*(datetime.datetime.now()-dt0)/(idx+1))[:4] + \n",
    "                    '           ')\n",
    "    sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_to_file(new_dict,file_name):\n",
    "    exists = os.path.isfile(file_name)\n",
    "    \n",
    "    if type(new_dict) != list: new_dict = [new_dict]\n",
    "    \n",
    "    if exists:\n",
    "        with open(file_name) as ff:\n",
    "            json_new = json.load(ff)['key'] \n",
    "        json_new = json_new + new_dict\n",
    "    else:\n",
    "        json_new = new_dict\n",
    "    \n",
    "    json_new2 ={'key':json_new}\n",
    "    \n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(json_new2, f, ensure_ascii=False)\n",
    "        \n",
    "def load_file(file_name):\n",
    "    exists = os.path.isfile(file_name)\n",
    "    if exists:\n",
    "        with open(file_name) as ff:\n",
    "            json_new = json.load(ff)['key'] \n",
    "    else:\n",
    "        json_new = []\n",
    "    \n",
    "    return json_new\n",
    "\n",
    "def write_file(new_dict,file_name):\n",
    "    if type(new_dict) != list: new_dict = [new_dict]\n",
    "    \n",
    "    json_new2 ={'key':new_dict}\n",
    "    \n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(json_new2, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Ticker Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>KG_Search</th>\n",
       "      <th>KG_ID</th>\n",
       "      <th>KG_Desc</th>\n",
       "      <th>Description</th>\n",
       "      <th>Comp_URL</th>\n",
       "      <th>Wiki_URL</th>\n",
       "      <th>KG_nr_hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM.N</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>/m/0h1jr</td>\n",
       "      <td>Manufacturing company</td>\n",
       "      <td>['3M Company (3M), incorporated on June 25, 19...</td>\n",
       "      <td>http://www.3m.com/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/3M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS.N</td>\n",
       "      <td>A. O. Smith Corp</td>\n",
       "      <td>/m/03d3zfb</td>\n",
       "      <td>Manufacturing company</td>\n",
       "      <td>[\"A. O. Smith Corporation, incorporated on Jul...</td>\n",
       "      <td>http://www.aosmith.com/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A._O._Smith</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT.N</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>/m/02gkg4</td>\n",
       "      <td>Pharmaceutical company</td>\n",
       "      <td>['Abbott Laboratories, incorporated on March 6...</td>\n",
       "      <td>http://www.abbott.com</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbott_Laboratories</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV.N</td>\n",
       "      <td>AbbVie Inc</td>\n",
       "      <td>/m/0rzs09c</td>\n",
       "      <td>Company</td>\n",
       "      <td>[\"AbbVie Inc. (AbbVie), incorporated on April ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/AbbVie_Inc.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABMD.OQ</td>\n",
       "      <td>Abiomed Inc</td>\n",
       "      <td>/m/0gtn3k</td>\n",
       "      <td>Medical device company</td>\n",
       "      <td>[\"ABIOMED, Inc., incorporated on June 4, 1987,...</td>\n",
       "      <td>http://www.abiomed.com/</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abiomed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker            KG_Search       KG_ID                 KG_Desc  \\\n",
       "0    MMM.N                3M Co    /m/0h1jr   Manufacturing company   \n",
       "1    AOS.N     A. O. Smith Corp  /m/03d3zfb   Manufacturing company   \n",
       "2    ABT.N  Abbott Laboratories   /m/02gkg4  Pharmaceutical company   \n",
       "3   ABBV.N           AbbVie Inc  /m/0rzs09c                 Company   \n",
       "4  ABMD.OQ          Abiomed Inc   /m/0gtn3k  Medical device company   \n",
       "\n",
       "                                         Description                 Comp_URL  \\\n",
       "0  ['3M Company (3M), incorporated on June 25, 19...       http://www.3m.com/   \n",
       "1  [\"A. O. Smith Corporation, incorporated on Jul...  http://www.aosmith.com/   \n",
       "2  ['Abbott Laboratories, incorporated on March 6...    http://www.abbott.com   \n",
       "3  [\"AbbVie Inc. (AbbVie), incorporated on April ...                      NaN   \n",
       "4  [\"ABIOMED, Inc., incorporated on June 4, 1987,...  http://www.abiomed.com/   \n",
       "\n",
       "                                            Wiki_URL  KG_nr_hit  \n",
       "0                   https://en.wikipedia.org/wiki/3M          0  \n",
       "1          https://en.wikipedia.org/wiki/A._O._Smith          0  \n",
       "2  https://en.wikipedia.org/wiki/Abbott_Laboratories          0  \n",
       "3          https://en.wikipedia.org/wiki/AbbVie_Inc.          0  \n",
       "4              https://en.wikipedia.org/wiki/Abiomed          0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk_list = 'SnP'\n",
    "\n",
    "tick_df=pd.read_csv('./Ticker_TO_KG_ID_Map/'+stk_list+'_Ticker_to_KG.csv',index_col=0)\n",
    "\n",
    "stk_ticks=list(tick_df.Ticker)\n",
    "stk_kg_id=list(tick_df.KG_ID)\n",
    "stk_name=list(tick_df.KG_Search)\n",
    "tick_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path='./entities/'\n",
    "\n",
    "list_files=['rss_crawl2019-04-26 04-03-25_entities.txt',\n",
    "       'rss_crawl2019-04-25 23-00-26_entities.txt', \n",
    "       'rss_crawl2019-04-10 08-45-38_entities.txt',\n",
    "       'rss_crawl2019-04-13 02-07-56_entities.txt',\n",
    "       'rss_crawl2019-04-23 07-51-54_entities.txt',\n",
    "       'rss_crawl2019-04-20 09-10-00_entities.txt',\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for r, d, f in os.walk(path):\n",
    "    list_files=f\n",
    "\n",
    "list_files=list(set(list_files)-{'.DS_Store'})\n",
    "len(list_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Stock Ticker Files\n",
    "\n",
    "The loop reuns through each News file and allocates any news that match a KG ID from a provided list of KG_IDs and a list of files to go through. Any hit against a KG_ID gets written to \n",
    " 1. A file with the full news article in the same format\n",
    " 2. An Index file ending in _idx_ which stores the ids of all matching articles\n",
    "\n",
    "INPUTS:\n",
    " 1. stk_kg_id - List of KG_IDs to search\n",
    " 2. stk_ticks - List of stock tickers\n",
    " 3. list_files - list of files to search through\n",
    " 3. salience_threshold - Minimal relevancy of the the stock to an article\n",
    " \n",
    "BENEFITS:\n",
    "\n",
    "This setup has a few useful benefits\n",
    " 1. Does not rely on keeping track - the index files keep track for you of which news were included so even if a file has been run through twice the news will only exist once\n",
    " 2. Can be flexible in changing the list of stocks or list of news files\n",
    " 3. One can use this for a daily update and a big batch update where the day's file is just dumped here and allocated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Improved Design\n",
    "\n",
    "The current design runs through each news file and for each hit loads the stock and the index file, saves the hit and moves on. This is not efficient as loading and closing the files takes a long time, especially when stock files start to get bigger.\n",
    "\n",
    "Improved Design: \n",
    " 1. Change the index file to be a central index file which is a dictionary by stock ticker and holds all news IDs by Stock ticker to look up presence\n",
    " 2. Per each news file, a small dictionary by stock ticker with matching content from the current file will be added to. \n",
    " 3. When done with a file, the dictionary will allocate all results from the file to news files at one go and store the new version of the index file. News distionary will be reset for the next file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: rss_crawl2019-04-14 01-21-16_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-24 18-09-52_entities.txt\n",
      "5200/5200  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-22 12-30-19_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-14 18-20-27_entities.txt\n",
      "5000/5000  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-24 07-59-16_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:02           Loaded: rss_crawl2019-04-12 22-54-42_entities.txt\n",
      "5000/5000  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-12 03-37-18_entities.txt\n",
      "7050/7050  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-22 00-34-30_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:02           Loaded: rss_crawl2019-04-02 205174_entities.txt\n",
      "1038/1038  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-10 08-45-38_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:02           Loaded: rss_crawl2019-04-03 232220050155_entities.txt\n",
      "118/118  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-20 01-30-29_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:17           Loaded: rss_crawl2019-04-11 22-44-38_entities.txt\n",
      "2000/2000  Progress 100.00% |====================>|ETA: 0:00/0:03           Loaded: rss_crawl2019-04-21 15-13-08_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:24           Loaded: rss_crawl2019-04-14 00-07-39_entities.txt\n",
      "2000/2000  Progress 100.00% |====================>|ETA: 0:00/0:03           Loaded: rss_crawl2019-04-09 20-01-59_entities.txt\n",
      "942/942  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-09 20-24-37_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:23           Loaded: rss_crawl2019-04-21 01-14-50_entities.txt\n",
      "200/200  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-03-25 224607.300145_entities.txt\n",
      "141/141  Progress 100.00% |====================>|ETA: 0:00/0:03           Loaded: rss_crawl2019-04-25 08-11-10_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:29           Loaded: rss_crawl2019-04-20 12-21-05_entities.txt\n",
      "2000/2000  Progress 100.00% |====================>|ETA: 0:00/0:06           Loaded: rss_crawl2019-04-21 14-08-57_entities.txt\n",
      "1600/1600  Progress 100.00% |====================>|ETA: 0:00/0:04           Loaded: rss_crawl2019-04-19 12-33-11_entities.txt\n",
      "1250/1250  Progress 100.00% |====================>|ETA: 0:00/0:03           Loaded: rss_crawl2019-04-08 21-55-33_entities.txt\n",
      "903/903  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-03-25 133527.717086_entities.txt\n",
      "35/35  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-03-26 192945.366840_entities.txt\n",
      "883/883  Progress 100.00% |====================>|ETA: 0:00/0:02           Loaded: rss_crawl2019-04-23 17-04-16_entities.txt\n",
      "7450/7450  Progress 100.00% |====================>|ETA: 0:00/0:21           Loaded: rss_crawl2019-04-25 23-00-26_entities.txt\n",
      "8450/8450  Progress 100.00% |====================>|ETA: 0:00/0:08           Loaded: rss_crawl2019-04-13 07-47-22_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:27           Loaded: rss_crawl2019-04-23 00-22-20_entities.txt\n",
      "7950/7950  Progress 100.00% |====================>|ETA: 0:00/0:29           Loaded: rss_crawl2019-04-22 08-25-28_entities.txt\n",
      "6800/6800  Progress 100.00% |====================>|ETA: 0:00/0:24           Loaded: rss_crawl2019-04-26 04-03-25_entities.txt\n",
      "8477/8477  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-23 07-51-54_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:09           Loaded: rss_crawl2019-03-25 164055.644673_entities.txt\n",
      "74/74  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-14 11-15-08_entities.txt\n",
      "5000/5000  Progress 100.00% |====================>|ETA: 0:00/0:15           Loaded: rss_crawl2019-04-12 12-28-35_entities.txt\n",
      "3450/3450  Progress 100.00% |====================>|ETA: 0:00/0:12           Loaded: rss_crawl2019-04-23 21-24-38_entities.txt\n",
      "3000/3000  Progress 100.00% |====================>|ETA: 0:00/0:11           Loaded: rss_crawl2019-04-05 22-30-16_entities.txt\n",
      "294/294  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-19 13-16-01_entities.txt\n",
      "8800/8800  Progress 100.00% |====================>|ETA: 0:00/0:38           Loaded: rss_crawl2019-04-11 01-44-33_entities.txt\n",
      "923/923  Progress 100.00% |====================>|ETA: 0:00/0:02           Loaded: rss_crawl2019-04-23 23-09-11_entities.txt\n",
      "3000/3000  Progress 100.00% |====================>|ETA: 0:00/0:13           Loaded: rss_crawl2019-03-25 140722.746408_entities.txt\n",
      "47/47  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-12 16-23-53_entities.txt\n",
      "3600/3600  Progress 100.00% |====================>|ETA: 0:00/0:13           Loaded: rss_crawl2019-03-29 00.774701_entities.txt\n",
      "1249/1249  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-19 18-11-00_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:44           Loaded: rss_crawl2019-03-25 183.490591_entities.txt\n",
      "59/59  Progress 100.00% |====================>|ETA: 0:00/0:02           Loaded: rss_crawl2019-04-21 01-25-47_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:56           Loaded: rss_crawl2019-04-20 09-10-00_entities.txt\n",
      "5500/5500  Progress 100.00% |====================>|ETA: 0:00/0:07           Loaded: rss_crawl2019-04-13 18-00-18_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:38           Loaded: rss_crawl2019-03-26 220209.918207_entities.txt\n",
      "114/114  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-19 23-49-12_entities.txt\n",
      "3000/3000  Progress 100.00% |====================>|ETA: 0:00/0:14           Loaded: rss_crawl2019-04-06 01-22-34_entities.txt\n",
      "54/54  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-03-25 173830.805708_entities.txt\n",
      "9/9  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-03-25 160644.577845_entities.txt\n",
      "185/185  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-09 00-24-58.063131_entities.txt\n",
      "100/100  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-04 195840189138_entities.txt\n",
      "954/954  Progress 100.00% |====================>|ETA: 0:00/0:04           Loaded: rss_crawl2019-04-12 08-14-36_entities.txt\n",
      "6250/6250  Progress 100.00% |====================>|ETA: 0:00/0:32           Loaded: rss_crawl2019-04-14 14-53-25_entities.txt\n",
      "5000/5000  Progress 100.00% |====================>|ETA: 0:00/0:20           Loaded: rss_crawl2019-03-25 173708.153802_entities.txt\n",
      "165/165  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-10 15-24-49_entities.txt\n",
      "2000/2000  Progress 100.00% |====================>|ETA: 0:00/0:08           Loaded: rss_crawl2019-04-05 01550682044_entities.txt\n",
      "63/63  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-12 19-47-31_entities.txt\n",
      "5000/5000  Progress 100.00% |====================>|ETA: 0:00/0:20           Loaded: rss_crawl2019-04-20 19-05-24_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:48           Loaded: rss_crawl2019-03-29 0085612_entities.txt\n",
      "4/4  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-05 13-09-32_entities.txt\n",
      "43/43  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-24 07-08-31_entities.txt\n",
      "350/350  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-04-03 1951442_entities.txt\n",
      "914/914  Progress 100.00% |====================>|ETA: 0:00/0:04           Loaded: rss_crawl2019-04-24 21-41-28_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:58           Loaded: rss_crawl2019-04-05 10-42-45_entities.txt\n",
      "343/343  Progress 100.00% |====================>|ETA: 0:00/0:01           Loaded: rss_crawl2019-03-29 000445.054593_entities.txt\n",
      "1018/1018  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-09 01-46-45_entities.txt\n",
      "2000/2000  Progress 100.00% |====================>|ETA: 0:00/0:11           Loaded: rss_crawl2019-04-05 15-43-29_entities.txt\n",
      "195/195  Progress 100.00% |====================>|ETA: 0:00/0:00           Loaded: rss_crawl2019-04-10 17-26-47_entities.txt\n",
      "10000/10000  Progress 100.00% |====================>|ETA: 0:00/0:53           Loaded: rss_crawl2019-03-25 010027.315286_entities.txt\n",
      "575/575  Progress 100.00% |====================>|ETA: 0:00/0:12           Loaded: rss_crawl2019-03-25 130307.329902_entities.txt\n",
      "186/186  Progress 100.00% |====================>|ETA: 0:00/0:04           Loaded: rss_crawl2019-04-04 231136018255_entities.txt\n",
      "134/134  Progress 100.00% |====================>|ETA: 0:00/0:04           Loaded: rss_crawl2019-04-11 13-25-22_entities.txt\n",
      "81/10000  Progress 0.81% |>-------------------|ETA: 1:30/1:31           "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e7217527f348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0midx_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_file_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_idx.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mjson2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0madd_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_file_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                     \u001b[0madd_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_file_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_idx.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mnr_hits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnr_hits\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-fb1bee901f59>\u001b[0m in \u001b[0;36madd_to_file\u001b[0;34m(new_dict, file_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_new2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MD/anaconda/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "salience_threshold = 0.005\n",
    "path_store = './stk_news_list/'\n",
    "\n",
    "\n",
    "nr_hits = 0\n",
    "ttl_hits = 0\n",
    "for file in list_files:\n",
    "    with open(path+file) as ff:\n",
    "        json2 = json.load(ff)['key']\n",
    "    print('Loaded: ' + file)\n",
    "    dt0 = datetime.datetime.now()\n",
    "    for idx in range(len(json2)):\n",
    "        if len(json2[idx].get('entities',''))>0:\n",
    "            kg_ids_art=[json2[idx]['entities'][loop]['mid'] \n",
    "                        for loop in range(len(json2[idx]['entities']))\n",
    "                        if json2[idx]['entities'][loop].get('salience',0)>salience_threshold]\n",
    "\n",
    "            stk_news_intersect = list(set(stk_kg_id) & set(kg_ids_art))\n",
    "            for stk_kg_itrs in stk_news_intersect:\n",
    "                #print(stk_kg_itrs)\n",
    "                loc_in_list = stk_kg_id.index(stk_kg_itrs)\n",
    "\n",
    "                new_file_name = path_store+stk_ticks[loc_in_list]\n",
    "                \n",
    "                idx_file = load_file(new_file_name+'_idx.txt')\n",
    "                if json2[idx]['id'] not in idx_file:\n",
    "                    add_to_file(json2[idx],new_file_name+'.txt')\n",
    "                    add_to_file(json2[idx]['id'],new_file_name+'_idx.txt')\n",
    "                nr_hits = nr_hits +1\n",
    "        prgs_bar(dt0, idx, len(json2), scale=20)\n",
    "        ttl_hits = ttl_hits + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded: rss_crawl2019-04-09 00-24-58.063131_entities.txt\n",
      "100/100  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-13 18-00-18_entities.txt\n",
      "9786/10000   Progress 97.86% |===================>|ETA: 0:00/0:00           \n",
      "Loaded: rss_crawl2019-04-25 23-00-26_entities.txt\n",
      "8450/8450  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-05 13-09-32_entities.txt\n",
      "43/43  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-05 22-30-16_entities.txt\n",
      "294/294  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-12 16-23-53_entities.txt\n",
      "3600/3600  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-04 231136018255_entities.txt\n",
      "134/134  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-11 22-28-45_entities.txt\n",
      "839/839  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-05 01550682044_entities.txt\n",
      "63/63  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-25 183.490591_entities.txt\n",
      "59/59  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-25 173708.153802_entities.txt\n",
      "165/165  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-22 08-25-28_entities.txt\n",
      "6800/6800  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-26 04-03-25_entities.txt\n",
      "8477/8477  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-19 13-16-01_entities.txt\n",
      "8800/8800  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-24 07-08-31_entities.txt\n",
      "350/350  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-29 0085612_entities.txt\n",
      "4/4  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-25 010027.315286_entities.txt\n",
      "575/575  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-25 140722.746408_entities.txt\n",
      "47/47  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-24 21-41-28_entities.txt\n",
      "10000/10000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-21 15-13-08_entities.txt\n",
      "10000/10000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-24 00-49-39_entities.txt\n",
      "10000/10000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-14 01-21-16_entities.txt\n",
      "10000/10000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-10 17-26-47_entities.txt\n",
      "10000/10000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-29 000445.054593_entities.txt\n",
      "1018/1018  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-03 1951442_entities.txt\n",
      "914/914  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-23 23-09-11_entities.txt\n",
      "3000/3000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-29 00.947101_entities.txt\n",
      "1250/1250  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-05 10-42-45_entities.txt\n",
      "343/343  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-26 220209.918207_entities.txt\n",
      "114/114  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-25 164055.644673_entities.txt\n",
      "74/74  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-11 01-44-33_entities.txt\n",
      "923/923  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-25 173830.805708_entities.txt\n",
      "9/9  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-03-25 160644.577845_entities.txt\n",
      "185/185  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-25 17-29-10_entities.txt\n",
      "9400/9400  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-13 02-07-56_entities.txt\n",
      "7000/7000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-21 14-08-57_entities.txt\n",
      "1600/1600  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n",
      "\n",
      "Loaded: rss_crawl2019-04-11 13-25-22_entities.txt\n",
      "10000/10000  Progress 100.00% |====================|ETA: 0:00/0:00           Saving...\n"
     ]
    }
   ],
   "source": [
    "salience_threshold = 0.005\n",
    "path_store = './stk_news_list/'\n",
    "\n",
    "\n",
    "nr_hits = 0\n",
    "ttl_hits = 0\n",
    "\n",
    "\n",
    "\n",
    "for file in list_files:\n",
    "    # Open news file\n",
    "    with open(path+file) as ff:\n",
    "        json2 = json.load(ff)['key']\n",
    "    print('\\nLoaded: ' + file)\n",
    "    \n",
    "    dt0 = datetime.datetime.now()\n",
    "    \n",
    "    # Create empty dictionaries for indices and news per file\n",
    "    # Index dictionary will be loaded with all existing indices\n",
    "    dict_idx={}\n",
    "    for stk in stk_ticks:\n",
    "        idx_file = load_file(path_store+stk+'_idx.txt')\n",
    "        if len(idx_file) > 0: dict_idx[stk] = idx_file\n",
    "    dict_nws={}\n",
    "    \n",
    "    \n",
    "    for idx in range(len(json2)):\n",
    "        if len(json2[idx].get('entities',''))>0:\n",
    "            kg_ids_art=[json2[idx]['entities'][loop]['mid'] \n",
    "                        for loop in range(len(json2[idx]['entities']))\n",
    "                        if json2[idx]['entities'][loop].get('salience',0)>salience_threshold]\n",
    "\n",
    "            stk_news_intersect = list(set(stk_kg_id) & set(kg_ids_art))\n",
    "            for stk_kg_itrs in stk_news_intersect:\n",
    "                #print(stk_kg_itrs)\n",
    "                loc_in_list = stk_kg_id.index(stk_kg_itrs)\n",
    "                stk_ticker = stk_ticks[loc_in_list]\n",
    "                \n",
    "\n",
    "                if json2[idx]['id'] not in dict_idx.get(stk_ticker,[]):\n",
    "                    dict_nws[stk_ticker]=dict_nws.get(stk_ticker,[]) + [json2[idx]]\n",
    "                    dict_idx[stk_ticker]=dict_idx.get(stk_ticker,[]) + [json2[idx]['id']]\n",
    "\n",
    "                nr_hits = nr_hits +1\n",
    "        prgs_bar(dt0, idx, len(json2), scale=20)\n",
    "        ttl_hits = ttl_hits + 1\n",
    "    \n",
    "    # Save down updated files at the end of the round\n",
    "    print('Saving...')\n",
    "    for stk in stk_ticks:\n",
    "        #assert(len(dict_idx.get(stk,[])) == len(dict_nws.get(stk,[])))\n",
    "        if len(dict_nws.get(stk,[]))>0:\n",
    "            add_to_file(dict_nws[stk],path_store+stk+'.txt')\n",
    "            write_file(dict_idx[stk],path_store+stk+'_idx.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MMM.N'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
